{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3879648-1463-4288-926f-5df79a5ad953",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import pickle\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "\n",
    "class TrainModel:\n",
    "    \"\"\" \n",
    "            Initializes the training object. \n",
    "\n",
    "            Parameters: \n",
    "                param_grid (dict) : Dictionary of the parameter grid supplied by the user \n",
    "                search_strategy (str) : Hyperparameter tuning strategy to optimize the given parameter grid. \n",
    "                model_dictionary (dict) : Dictionary of models. \n",
    "                performance_measure (str): Performance to be considered in the hyperparameter tuning.\n",
    "                cv (int) : Amount of cross validation.\n",
    "                n_jobs (int) : Concurrent processes that are being used. \n",
    "                verbose (int) : Extra information during the process.\n",
    "                threshold (float) : Threshold for prediction.\n",
    "                calibration_needed (str): If yes, calibration will be applied. \n",
    "                interaction (str): Interaction term for logistic regression.\n",
    "                dimensionality_reduction (str): Type of dimensionality reduction. \n",
    "                n_components(int): Number of components in dimensionality reduction technique.\n",
    "                \n",
    "        \"\"\"\n",
    "    def __init__(self, param_grid = None, search_strategy = \"grid\", model_dictionary = None,\n",
    "                performance_measure = \"f1\", cv = 3, n_jobs = -1, verbose = 1, \n",
    "                 threshold = None, calibration_needed=None, interaction=None, dimensionality_reduction=None, n_components = None):\n",
    "        \n",
    "    \n",
    "        self.search_strategy = search_strategy\n",
    "        self.performance_measure = performance_measure\n",
    "        self.cv = cv\n",
    "        self.n_jobs = n_jobs\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.param_grid = param_grid\n",
    "\n",
    "        self.model_dictionary = model_dictionary\n",
    "        self.threshold = threshold\n",
    "        self.calibration_needed=calibration_needed\n",
    "        self.interaction = interaction\n",
    "        self.dimensionality_reduction = dimensionality_reduction\n",
    "        self.n_components = n_components\n",
    "        self.best_estimator_b = None\n",
    "        self.best_score_b = None\n",
    "        self.best_model = None\n",
    "        \n",
    "        \n",
    "\n",
    "    def fitting_every_model(self, X, y, preprocessor, imbalance_handler = \"none\"):\n",
    "        \n",
    "        \"\"\" \n",
    "            Fits every model stated in the model dictionary.\n",
    "\n",
    "            Parameters: \n",
    "                X (data frame) : Training data for X. \n",
    "                y (data series) : Training data for y. \n",
    "                imbalance_handler (str) : Imbalance handler technique that is taken from imbalancehandler class. \n",
    "                preprocessor (object): Preprocesser that comes from data gathering module. \n",
    "            Returns: \n",
    "                Returns best estimator. \n",
    "        \"\"\"\n",
    "        scores = {}\n",
    "        performance_list = {}\n",
    "        best_score_o = -1000\n",
    "        best_estimator = None\n",
    "        best_model = None\n",
    "     \n",
    "        sampler_ = imbalance_handler.sampler_(y)\n",
    "        \n",
    "        poly = PolynomialFeatures(degree=2,\n",
    "                          interaction_only=True)\n",
    "\n",
    "        for name, models in self.model_dictionary.items():\n",
    "\n",
    "            starting_model = models()\n",
    "            grid = self.param_grid[name].copy() \n",
    "            steps= []\n",
    "            \n",
    "            if preprocessor is not None: \n",
    "                \n",
    "                steps.append((\"preprocessor\", preprocessor))\n",
    "            \n",
    "            if (name==\"logistic_regression\") and (self.interaction is not None):\n",
    "                 \n",
    "                steps.append((\"poly\", poly))\n",
    "\n",
    "            if self.dimensionality_reduction == \"pca\": \n",
    "                steps.append((\"pca\", PCA(n_components = self.n_components)))\n",
    "            elif self.dimensionality_reduction == \"umap\": \n",
    "                steps.append((\"tsne\", umap.UMAP(n_neighbors = self.n_components)))\n",
    "\n",
    "            else: \n",
    "                pass\n",
    "            \n",
    "            if sampler_ is not None:\n",
    "                \n",
    "                if imbalance_handler.method == \"classweights\": \n",
    "                    steps.append((name, starting_model))\n",
    "                    model = Pipeline(steps=steps)\n",
    "                    grid = {f\"{name}__{param_name}\": param_value for param_name, param_value in grid.items()}\n",
    "                    grid[f\"{name}__class_weight\"]= [sampler_]\n",
    "\n",
    "                else: \n",
    "                    steps.append((\"sampler\", sampler_))\n",
    "                    steps.append((name, starting_model))\n",
    "                    \n",
    "                    model = Pipeline(\n",
    "                       steps=steps)\n",
    "\n",
    "                    grid = {f\"{name}__{param_name}\": param_value for param_name, param_value in grid.items()}\n",
    "                    \n",
    "            \n",
    "            else:\n",
    "                steps.append((name, starting_model))\n",
    "                model = Pipeline(steps=steps)\n",
    "                grid = {f\"{name}__{param_name}\": param_value for param_name, param_value in grid.items()}\n",
    "                \n",
    "            if name == \"adaboost\": \n",
    "                print(\"I am using AdaBoost !\")\n",
    "                if isinstance(model, Pipeline):  \n",
    "                    grid[f\"{name}__estimator\"] = [SVC(C = 1, kernel=\"rbf\"),\n",
    "                                                 LogisticRegression(penalty=\"l2\"),\n",
    "                                                 RandomForestClassifier(n_estimators = 300)]\n",
    "                else:\n",
    "                    grid[\"estimator\"] = [SVC(C = 1, kernel=\"rbf\"),\n",
    "                                         LogisticRegression(penalty=\"l2\"),\n",
    "                                         RandomForestClassifier(n_estimators = 300)]      \n",
    "            \n",
    "            \n",
    "            if self.search_strategy == \"grid\":\n",
    "                print(model, grid)\n",
    "                opt = GridSearchCV(model, grid, scoring=self.performance_measure, \n",
    "                               cv = self.cv, n_jobs = self.n_jobs, verbose = self.verbose)\n",
    "    \n",
    "            elif self.search_strategy == \"random\": \n",
    "                print(model, grid)\n",
    "                opt = RandomizedSearchCV(model, grid, scoring = self.performance_measure,\n",
    "                                    cv = self.cv, n_jobs = self.n_jobs, verbose = self.verbose)\n",
    "            \n",
    "            opt.fit(X, y)\n",
    "            performance_list[name] = opt\n",
    "            scores[name] = opt.best_score_\n",
    "            \n",
    "        print(\"*****************************************\")\n",
    "        print(\"Best scores for each model\")\n",
    "        print(performance_list)\n",
    "        print(f\"Scores **************** {scores}\")\n",
    "    \n",
    "        best_model = max(performance_list, key= lambda k: performance_list[k].best_score_) \n",
    "        best_score_o = performance_list[best_model].best_score_\n",
    "        best_estimator = performance_list[best_model].best_estimator_\n",
    "        \n",
    "        self.best_estimator_b = best_estimator\n",
    "        self.best_score_b = best_score_o\n",
    "        self.best_model = best_model\n",
    "        \n",
    "        print(f\"Best model: {best_model} with respect to {self.performance_measure} of {performance_list[best_model]}\")\n",
    "        return self.best_estimator_b\n",
    "            \n",
    "\n",
    "    def calibration(self, X_train, y_train, X_calib, y_calib):\n",
    "        \"\"\" \n",
    "            Calibration of the probabilities.\n",
    "\n",
    "            Parameters: \n",
    "                X_train (data frame) : Training data for X. \n",
    "                y_train (data series) : Training data for y. \n",
    "                X_calib (data frame) : Calibration data for X. \n",
    "                y_calib (data series) : Calibration data for y. \n",
    "            \n",
    "            Returns: \n",
    "                Returns best estimator with or without calibration applied. \n",
    "        \"\"\"\n",
    "        best_ = self.best_estimator_b\n",
    "        best_.fit(X_train, y_train)\n",
    "        \n",
    "        if hasattr(best_, \"predict_proba\"): \n",
    "\n",
    "            y_prob = self.best_estimator_b.predict_proba(X_calib)[:, 1]\n",
    "            true, pred = calibration_curve(y_calib, y_prob)\n",
    "            plt.plot([0, 1], [0, 1], linestyle = \"--\")\n",
    "            plt.plot(pred, true)\n",
    "            plt.show()\n",
    "\n",
    "        else: \n",
    "            y_prob = self.best_estimator_b.decision_function(X_calib)\n",
    "            true, pred = calibration_curve(y_calib, y_prob, n_bins= 10)\n",
    "            plt.plot([0, 1], [0, 1], linestyle = \"--\")\n",
    "            plt.plot(pred, true)\n",
    "            plt.show()\n",
    "            \n",
    "         \n",
    "        if self.calibration_needed == \"yes\": \n",
    "            if self.best_model != \"logistic_regression\":\n",
    "                \n",
    "                if self.best_model == \"svm\":\n",
    "                    \n",
    "                    calibrated = CalibratedClassifierCV(best_, method = \"sigmoid\", cv = \"prefit\")\n",
    "    \n",
    "                else:\n",
    "                    calibrated = CalibratedClassifierCV(best_, method = \"isotonic\", cv = \"prefit\")\n",
    "    \n",
    "                calibrated.fit(X_calib, y_calib)\n",
    "                self.best_estimator_b = calibrated\n",
    "            \n",
    "                return self.best_estimator_b\n",
    "            \n",
    "            else: \n",
    "                return best_\n",
    "        else: \n",
    "            return best_ \n",
    "            \n",
    "    def predicting_model(self, X, X_train, y_train, X_calib, y_calib):\n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "            Predicts with using X return. \n",
    "\n",
    "            Parameters: \n",
    "                X_train (data frame) : Training data for X. \n",
    "                y_train (data series) : Training data for y. \n",
    "                X_calib (data frame) : Calibration data for X. \n",
    "                y_calib (data series) : Calibration data for y. \n",
    "                \n",
    "            Returns: \n",
    "                y_proba (data frame): Data frame that describes probabilities of y. \n",
    "                y_test (data series): Data series that indicates whether the entry is 0 or 1. \n",
    "        \"\"\"\n",
    "        \n",
    "        calibrated_estimator = self.calibration(X_train, y_train, X_calib, y_calib)\n",
    "        y_proba = None\n",
    "        y_test = None\n",
    "        \n",
    "        if calibrated_estimator is not None:\n",
    "            clf = calibrated_estimator          \n",
    "            if isinstance(clf, CalibratedClassifierCV):\n",
    "                pipe = clf.estimator          \n",
    "            else:\n",
    "                pipe = clf\n",
    "            \n",
    "            \n",
    "            \n",
    "            prep = pipe.named_steps[\"preprocessor\"]\n",
    "            X_ = prep.transform(X) \n",
    "            last_m = pipe.steps[-1][1]\n",
    "            if hasattr(last_m, \"predict_proba\"): \n",
    "                \n",
    "                y_proba = self.best_estimator_b.predict_proba(X)\n",
    "                y_test = self.best_estimator_b.predict(X)\n",
    "                m = last_m.predict_proba\n",
    "                \n",
    "            else: \n",
    "                y_proba = None\n",
    "                y_test = self.best_estimator_b.predict(X)\n",
    "                m = last_m.predict\n",
    "               \n",
    "\n",
    "            print(\"Shapley Values ************ \")\n",
    "            \n",
    "            \n",
    "            if hasattr(last_m, \"coef_\"):\n",
    "                importances = last_m.coef_[0]\n",
    "            \n",
    "            if hasattr(last_m, \"feature_importances_\"):\n",
    "                importances = last_m.feature_importances_\n",
    "\n",
    "            if self.dimensionality_reduction is None: \n",
    "                \n",
    "                feat_names = prep.get_feature_names_out()\n",
    "                feature_imp = pd.DataFrame({\"Column\": feat_names, \"Importance\": importances})\n",
    "                sorted_imp = feature_imp.sort_values(\"Importance\", ascending=True)\n",
    "                sorted_imp.plot(x = \"Column\", y=\"Importance\", kind=\"barh\", figsize=(10,6))\n",
    "                plt.show()\n",
    "        \n",
    "                #cols = prep.get_feature_names_out()\n",
    "                #shapley_values = shap.Explainer(m, X_, feature_names = cols) \n",
    "                #scores = shapley_values(X_) \n",
    "                #shap.summary_plot(scores.values[..., 1], X_)\n",
    "\n",
    "                \n",
    "                \n",
    "            return y_proba, y_test\n",
    "\n",
    "    def analysis(self, X, y, X_train, y_train, X_calib, y_calib):\n",
    "        \"\"\"\n",
    "            Predicts with using X return. \n",
    "\n",
    "            Parameters:\n",
    "                X (data frame): Data for X. \n",
    "                y (data frame): Data for y.\n",
    "                X_train (data frame) : Training data for X. \n",
    "                y_train (data series) : Training data for y. \n",
    "                X_calib (data frame) : Calibration data for X. \n",
    "                y_calib (data series) : Calibration data for y. \n",
    "                \n",
    "            Returns: \n",
    "                Returns a confusion Matrix. \n",
    "        \"\"\"\n",
    "\n",
    "        y_proba, y_test_pred = self.predicting_model(X, X_train, y_train, X_calib, y_calib)\n",
    "\n",
    "        if y_proba is not None: \n",
    "            prediction_l = [1 if proba > self.threshold else 0 for proba in y_proba[:, 1]]\n",
    "            prediction = np.array(prediction_l)\n",
    "\n",
    "        else:\n",
    "            prediction = y_test_pred\n",
    "\n",
    "        cm = confusion_matrix(y, prediction)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.ylabel(\"Actual Class\")\n",
    "        plt.xlabel(\"Predicted Class\")\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "\n",
    "    def store(self, operation=None, file_name = None):\n",
    "        \n",
    "        \"\"\" \n",
    "            A method for storing or loading the model.\n",
    "\n",
    "            Parameters: \n",
    "                operation (str) : Name of the operation. \n",
    "                path (str) : Name of the path.\n",
    "\n",
    "             Returns:\n",
    "                 Returns loaded model.\n",
    "        \"\"\"\n",
    "        model = self.best_estimator_b \n",
    "        if operation == \"save\": \n",
    "            with open(file_name, \"wb\") as file: \n",
    "                pickle.dump(model, file)\n",
    "            return \"Succesfully saved\"\n",
    "            \n",
    "        elif operation == \"load\": \n",
    "            with open(file_name, \"rb\") as file: \n",
    "                load_model = pickle.load(file)\n",
    "\n",
    "            self.best_estimator_b = load_model\n",
    "            return load_model\n",
    "        else:\n",
    "            raise ValueError(\"Please enter save or load ! \")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9022fb85-d944-410d-85c1-a3c35455f533",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a257fd-6415-4f1d-aeb7-ddccec7e6ce5",
   "metadata": {},
   "source": [
    "1- Thrimanne. *Hyperparameter tuning using pipeline end to end ml part 2*. Accessed on March 3, 2025, from https://thrimanne.medium.com/hyperparameter-tuning-using-pipeline-end-to-end-ml-part-2-3-81c68e84d445\n",
    "        \n",
    "2- Medium Codex. *Building a mixed type preprocessing pipeline with scikilt learn*. Accessed on March 3, 2025, from https://medium.com/codex/building-a-mixed-type-preprocessing-pipeline-with-scikit-learn-f4d90f5919fa\n",
    "  \n",
    "3- Towards Data Science. *Hyperparameter tuning and sampling strategy*. Accessed on February 24, 2025, from https://towardsdatascience.com/hyperparameter-tuning-and-sampling-strategy-1014e05f6c14/\n",
    "        \n",
    "4- Kocur, A. *Hyperparameter tuning with pipelines*. Accessed on February 24, 2025, from https://medium.com/@kocur4d/hyper-parameter-tuning-with-pipelines-5310aff069d6\n",
    "        \n",
    "5- Kaggle. *Probability Calibration Tutorial*. Accessed on May 6, 2025, from https://www.kaggle.com/code/kelixirr/probability-calibration-tutorial\n",
    "\n",
    "6- Neptune.ai. *Brier score and model calibration*. Accessed on May 6, 2025, from https://neptune.ai/blog/brier-score-and-model-calibration\n",
    "\n",
    "7- Yannawut. *Get column name after fitting the machine learning pipeline*. Acessed on May 11, 2025, from https://yannawut.medium.com/get-column-name-after-fitting-the-machine-learning-pipeline-145a2a8051cc\n",
    "\n",
    "8- Towards Data Science. *Using SHAP values to explain how your machine learning model works*. Accessed on May 11, 2025, from https://towardsdatascience.com/using-shap-values-to-explain-how-your-machine-learning-model-works-732b3f40e137/\n",
    "\n",
    "9- ForecastEgy. *Feature importance in logistic regression*. Accessed on May 6, 2025, from https://forecastegy.com/posts/feature-importance-in-logistic-regression/\n",
    "\n",
    "10- Singh, A. *Hyperparameter tuning beyond the basics*. Accessed on March 7, 2025. https://medium.com/%40abhaysingh71711/hyperparameter-tuning-beyond-the-basics-34d36b014482"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env)",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
